{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EaqjihtwRrys"
   },
   "source": [
    "# Lab03: Web Crawler (Continue) & Information Retrieval.\n",
    "\n",
    "- MSSV: 19120689\n",
    "- Họ và tên: Lại Khánh Toàn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUG7FFTsRryt"
   },
   "source": [
    "## Yêu cầu bài tập\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; từ `TODO` cho biết những phần mà bạn cần phải làm.\n",
    "\n",
    "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
    "\n",
    "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
    "\n",
    "Sau đó, tạo thư mục có tên `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) Chép file notebook, file `t_data.txt` và file `raw_data` của các bạn (nếu file này kích thước lớn các bạn có thể chép link vào `link_data.txt`), nén thư mục `MSSV` này lại và nộp trên moodle.\n",
    "\n",
    "**Nội dung bài tập**\n",
    "\n",
    "Cài đặt một web crawler để thu thập dữ liệu từ: https://en.wikipedia.org/wiki/Web_mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HlhdT6BRryu"
   },
   "source": [
    "## Nội dung bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9-ZyiLjRryv"
   },
   "source": [
    "Cài đặt một Web crawler đơn giản bắt đầu từ URL: https://en.wikipedia.org/wiki/Web_mining, tìm liên kết và thu thập dữ liệu trong HTML tại URL này sau đó lặp lại với các URL vừa tìm được.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FJktAwbCOyod"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMSlOpSsRryv"
   },
   "source": [
    "## 1. Thu thập đường dẫn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZZ9lSUPRryw"
   },
   "source": [
    "- Robot.txt:  https://en.wikipedia.org/robots.txt\n",
    "- **Bước 1**: Thu thập đường dẫn từ https://en.wikipedia.org/wiki/Web_mining. Lưu trữ vào một danh sách `url_list`. \n",
    "- **Bước 2**: Lặp lại bước 1 cho các đường dẫn trong `url_list` (**lưu ý:** kiểm tra các đường dẫn vừa thu được đã nằm trong `url_list` hay không?). Dừng khi đã thu thập được 200 URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4HFWqw1VOrEe"
   },
   "outputs": [],
   "source": [
    "def get_urls(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    urls = []\n",
    "    \n",
    "    # TODO\n",
    "    # Lấy các url nằm trong trang web của url này, lưu lại vào biến urls\n",
    "    for link in soup.find_all('a', href= re.compile('http[s]?://(?:[-\\w.])+')):\n",
    "        urls.append(link.get('href'))\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_urls_recursive(start_url, limit):\n",
    "    urls = [start_url]\n",
    "    for url in urls:\n",
    "        # TODO\n",
    "        # Lấy các url nằm trong trang web của url này, lưu lại vào biến new_urls\n",
    "        # Với mỗi url mới trong new_urls:\n",
    "        #   Nếu nó chưa nằm trong urls thì thêm nó vô  \n",
    "        # Nếu kích thước của urls vượt quá limit thì dừng và xóa phần dư thừa\n",
    "        new_urls = get_urls(url)\n",
    "        for url in new_urls:\n",
    "            if url not in urls:\n",
    "                urls.append(url)\n",
    "        if len(urls) > limit:\n",
    "            urls = urls[:limit]\n",
    "            break\n",
    "    return urls\n",
    "url_list = get_urls_recursive('https://en.wikipedia.org/wiki/Web_mining', 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk4ty3jcRryx"
   },
   "source": [
    "## 2. Thu thập dữ liệu\n",
    "Thu thập dữ liệu từ `url_list`. Lưu trữ dữ liệu thu được vào dictionary data với keys là các từ, values gồm 2 phần tử: \n",
    "- `url_idx_list` với $idx \\in \\left[0,200\\right) \\cap \\mathbb{N}$\n",
    "- `frequency` \n",
    "    \n",
    "Ví dụ: `data['at']=[url_idx_list,frequency]`:\n",
    "- `url_idx_list`: danh sách các url mà trong dữ liệu của chúng (html document) chứa từ \"at\". \n",
    "- `frequency`: tần suất xuất hiện (số lần xuất hiện) của từ `at` trong dữ liệu của **tất cả đường dẫn thu được**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YB5nIZAhQr7-"
   },
   "outputs": [],
   "source": [
    "def text_filter(element):\n",
    "    \n",
    "    # TODO\n",
    "    # Cài đặt lại như Lab02\n",
    "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
    "        return False\n",
    "    elif isinstance(element, Comment):\n",
    "        '''Opinion mining?'''\n",
    "        return False\n",
    "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
    "        '''space, return, endline'''\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def wordList(url):\n",
    "    \n",
    "    # TODO\n",
    "    # Cài đặt lại như Lab02\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    text = soup.findAll(text=True)\n",
    "    filtered_text = list(filter(text_filter, text)) # list của các chuỗi\n",
    "    word_list = []\n",
    "\n",
    "    for strg in filtered_text: \n",
    "      new_strg = strg.replace(string.punctuation, \" \")\n",
    "      words = new_strg.split()\n",
    "      word_list.extend(words)\n",
    "\n",
    "    return word_list\n",
    "\n",
    "def read_url(url, url_idx, data):\n",
    "    # TODO\n",
    "    # Cài đặt lại như Lab02\n",
    "    word_list = wordList(url)\n",
    "\n",
    "    for word in word_list:\n",
    "      if word not in data:\n",
    "        data[word] = [[url_idx], 1]\n",
    "      else:\n",
    "        if url_idx not in data[word][0]:\n",
    "          data[word][0].append(url_idx)\n",
    "        data[word][1]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LrPuiiDhQfrJ"
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for url_index, url in enumerate(url_list, 1):\n",
    "    read_url(url, url_index, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "740eXy7pRryx"
   },
   "source": [
    "## 3. Tiền xử lý\n",
    "Loại bỏ các item trong data mà key là các stopword.\n",
    "\n",
    "**Ngữ liệu:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hC58K3Q3Rryy",
    "outputId": "99a20ea5-89c3-4d93-c74f-e43636b9b1cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cylo7trpRnun"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Loại bỏ các key của biến data mà nằm trong danh sách english_stopwords\n",
    "for val in list(data):\n",
    "    if val in english_stopwords:\n",
    "        del data[val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEoxNVHqRry2"
   },
   "source": [
    "## 4. Lưu trữ và biểu diễn dữ liệu\n",
    "Sử dụng pickle lưu lại data với tên file raw_data.\n",
    "### 4.1 Cơ sở dữ liệu giao tác:\n",
    "Thông thường, các cơ sở dữ liệu giao tác được lưu trong flat files (các tập phẳng) thay vì trong một hệ cơ sở dữ liệu. Các item là các số nguyên không âm, mỗi giao tác tương ứng với một dòng các số nguyên phân tách nhau bằng khoảng trắng.\n",
    "Ví dụ:\n",
    "\n",
    "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "32 69 \n",
    "\n",
    "48 70 71 72 \n",
    "\n",
    "39 73 74 75 76 77 78 79 \n",
    "\n",
    "36 38 39 41 48 79 80 81 \n",
    "\n",
    "82 83 84 \n",
    "\n",
    "41 85 86 87 88 \n",
    "\n",
    "39 48 89 90 91 92 93 94 95 96 97 98 99 100 101 \n",
    "\n",
    "36 38 39 48 89 \n",
    "\n",
    "39 41 102 103 104 105 106 107 108 \n",
    "\n",
    "38 39 41 109 110 \n",
    "\n",
    "39 111 112 113 114 115 116 117 118 \n",
    "\n",
    "119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 \n",
    "\n",
    "48 134 135 136 \n",
    "\n",
    "39 48 137 138 139 140 141 142 143 144 145 146 147 148 149 \n",
    "\n",
    "39 150 151 152 \n",
    "\n",
    "38 39 56 153 154 155 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4l--TVLEoN7R"
   },
   "outputs": [],
   "source": [
    "with open('raw_data', 'wb') as f:\n",
    "    # TODO\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvUgw0VkRry3"
   },
   "source": [
    "### 4.2 Xuất dataset\n",
    "Lưu một cơ sở dữ liệu giao tác (transactional database) vào file t_data.txt: \n",
    "- Các item tương ứng với url_idx\n",
    "- Mỗi transaction tương ứng với một từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZorK46LdSCfi"
   },
   "outputs": [],
   "source": [
    "with open('t_data.txt', 'w') as f:\n",
    "    for word, (url_list, freq) in data.items():\n",
    "        print(*url_list, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OXVFoHiRry3"
   },
   "source": [
    "## 5. Truy vấn and, or, not\n",
    "Ví dụ: \n",
    "- Truy vấn `and` câu `web mining`: trả về đường dẫn tới các trang web có cả 2 từ web và từ mining. \n",
    "- Truy vấn `or` câu `web mining`: trả về đường dẫn tới các trang web có từ web hoặc từ mining.\n",
    "- Truy vấn `not` câu `web mining`: trả về đường dẫn tới các trang không có cả từ web và từ mining.\n",
    "\n",
    "*GỢI Ý: TÁCH CÂU TRUY VẤN THÀNH CÁC TỪ TƯƠNG TỰ PHƯƠNG PHÁP LÀM Ở LAB02.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sybM0yLHRry6"
   },
   "outputs": [],
   "source": [
    "ret=[]\n",
    "def andRetrieval(ret, sentence):\n",
    "    '''Parameters\n",
    "    -----------------------\n",
    "    ret: url_list\n",
    "    sentence: query'''\n",
    "    # TODO\n",
    "    ### split sentence (separator ' ') into terms\n",
    "    ### find urls have all terms: urls\n",
    "    ### if len(ret)==0:  return urls\n",
    "    ### else update ret with urls: intersection of ret and urls \n",
    "    terms = sentence.split(' ')\n",
    "    \n",
    "    for term in terms:\n",
    "        if term not in data:\n",
    "            return []\n",
    "\n",
    "    urls = data[terms[0]][0]\n",
    "\n",
    "\n",
    "\n",
    "def orRetrieval(ret, sentence):\n",
    "    '''Parameters\n",
    "    -----------------------\n",
    "    ret: url_list\n",
    "    sentence: query'''\n",
    "    # TODO\n",
    "    ### split sentence (separator ' ') into terms\n",
    "    ### find urls have all terms: urls\n",
    "    ### find urls have at least 1 term: urls\n",
    "    ### update ret with urls: extend ret with urls\n",
    "\n",
    "def notRetrieval(ret, sentence):\n",
    "    '''Parameters\n",
    "    -----------------------\n",
    "    ret: url_list\n",
    "    sentence: query'''\n",
    "    # TODO\n",
    "    ### split sentence (separator ' ') into terms\n",
    "    ### find urls have at least 1 term: urls\n",
    "    ### update ret with urls: remove urls from ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hkp7ej3qxRQ"
   },
   "outputs": [],
   "source": [
    "print(andRetrieval([], 'web mining'))\n",
    "print(orRetrieval([], 'web mining'))\n",
    "print(notRetrieval(url_list, 'web mining'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab03-WebCrawler.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
